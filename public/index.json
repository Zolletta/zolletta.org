[{
    "uri": "http://localhost:1313/books/the-essence-of-software/",
    "title": "The Essence of Software: Why Concepts Matter for Great Design",
  "tags": ["modeling","conceptual-design","daniel-jackson","architecture"],
  "body": "Introduction In software engineering, we distinguish abstraction (the programming idea) and representation (the code). In the same way, when talking about user interaction, we can have:\nthe conceptual level: the abstraction. Comprises semantics, actions, data model, and purposes. the user interface: the representation, with : the physical level: e.g. colour, size, touch, accessibility, Fitt‚Äôs law‚Ä¶ the linguistic level : icons, labels, site structure, consistency. Must respect diversity of cultures and languages. While there were many studies concerning the user interfaces, few addressed the conceptual level. ‚ÄúThe essence of software‚Äù, a book by Daniel Jackson, professor of computer science at MIT, published by Princeton University Press, is one of those.\nFor Daniel Jackson,\n‚ÄúSoftware can be viewed as a systematic composition of concepts‚Äù.\nJackson is applying the conceptual-model framework to software design.\nWhy is this important? For two reasons.\nFormalism has been relegated for decades to human-computer interaction (HCI) studies. HCI is important, but is actually the study of the how we present something. Where is the study of the what we present? Choosing a perspective or another influences all the chain of interactions that can or cannot be achieved. Studying the concepts at the base of an app/software/company/whatever, forgetting about the implementation details because not relevant at that level, we can focus on the epistemic value of the concept and its interactions with other concepts. Engineering speaks of costs, performance, resilience: aspects that are important to humans but that usually stay invisible except when they fail.\nSimilarly, UX designers draft widgets, colours, fonts, and other aspects of the user interface that become conscious to humans only when something is not right (e.g. a mislabelled button or a confusing workflow).\nBoth groups have to work with data.\nDesigners, on the other hand, have to do with users‚Äô behaviours, and their analysis is qualitative. They shape users‚Äô goals so that they become resilient to their misdeeds while meeting their purposes.\nConcepts thus come before the other two levels: first we must understand the concepts at stake, and from different perspectives, and then we can design the software interface and/or the software architecture (and then proceed to the implementation).\nThe link between the conceptual level and the UX level is then provided by the concepts mapping, whereas a high decoupled OOP approach which leverages composition paves the way of eradicating complexity in software engineering.\nFormalism Through the Ages (Alloy) As a language for exploring designs, however, code is imperfect. It‚Äôs verbose and often indirect, and it does not allow partial descriptions in which some details are left to be resolved later. And testing, as a way to analyze designs, leaves much to be desired. It‚Äôs notoriously incomplete and burdensome, since you need to write test cases explicitly. And it‚Äôs very difficult to use code to articulate design without getting mired in low-level details (such as the choice of data representations). (source: https://cacm.acm.org/research/alloy/)\nIn the ‚Äò70 and ‚Äò80 several formal languages were developed by researchers in a quest for correctness, defined as the ability of a program to follow its specifications. **Some were ‚Äúmodel-based‚Äù, such as Z, VDM and B, where behaviour was a set of actions over a set of states (of sets and relations). Others were ‚Äúalgebraic‚Äù, such as OBJ and Larch where the behaviour was described using Observers (of the hidden state) and Mutators (of the hidden states).\nBut once researchers started writing formal specifications, they discovered that they weren‚Äôt so consistent and/or clear, after all: the very act of putting down specifications was an act of designing per se.\nAlloy was created taking all this into consideration, and was born explicitly for design explorations.\nAlloy uses the for-all and exists-some quantifiers of first-order logic with the operators of set theory and relational calculus.\nWith it, we can use e.dept. manager to get the manager of e‚Äòs department (or all the managers of the departments in which e is employed). We can also navigate backward, and this is because every value is a relation.\nAlloy uses a small scope analysis approach. Given that even a first-order logic (without relational operators) is not decidable, we must except some trade-off:\nwe can give up usefulness of language but making it decidable\nwe could forgo the automation, but then what‚Äôs the point in all this?\nwe could shrink scope, and that‚Äôs the approach Alloy is following. Neither abstraction nor simulation, however, but small tests.\nThe rationale for this is the small scope hypothesis, which asserts that most bugs can be demonstrated with small counterexamples. If you test for all small counterexamples, you are likely to find any bug. Many Alloy case studies have confirmed the hypothesis.‚Äù\nAlloy does not perform explicit searches because even with small scales the number of possibilities is astronomical. It treats this as a\n‚Äúsatisfiability problem whose variables are not relations but simple bits. By flipping bits individually, a satisfiability (SAT) solver can usually find a solution (if there is one) or show that none exists by examining only a tiny portion of the space. Alloy‚Äôs analysis tool is essentially a compiler to SAT, which allows it to exploit the latest advances in SAT solvers.\nGrowing a model in a declarative language like Alloy is very different from growing a program in a conventional programming language. A program starts with no behaviours at all, and as you add code, new behaviours become possible. With Alloy, it‚Äôs the opposite. The empty model, since it lacks any constraints, allows every possible behaviour; as you add constraints, behaviours are eliminated.\nBy running simulations in an incremental, agile way, you might face contradictions and faults that you can remove by narrowing down what the model can do.\nSource: https://cacm.acm.org/research/alloy/\nConcepts A concept is what the user needs to understand to use the software effectively.\nA concept is a solution to a design problem. Not a vague problem, but a well-defined, small problem that keeps occurring in many contexts.\nIn short, concepts are self-contained unit of functionality:\nAbstract Generic or polymorphic Favour reuse Independent They can work together (composed) for a larger goal Concepts embody experience.\nConcepts don‚Äôt make all design problems go away, of course. They do, however, help you localize challenges by recognising them as specific to a particular concept. A concept becomes a container not only for the behaviours that it embodies but also for the accumulated knowledge about its design, the issues that have arisen in practice as it has been deployed, and the various ways in which designers have dealt with them.\nWe should use concepts to shape the way designers think.\nGood design makes decisions seems inevitable because only one make sense, or because the common rules will lead the choice to the most acceptable one. And also avoids reinventing concepts for which there is already a solution not only because it is a waste of resource, but because it could confuse users since they are already used to a solution.\nFor example, what‚Äôs a cloud backup? A continuous backup of files in my local hard drive on my cloud (user‚Äôs perspective) or snapshots taken every given time (implementation‚Äôs point of view)?\nConcepts as vectors of differentiation To succeed, often an app or software delivers a brand-new concept (or a new way of understanding it):\nPhotoshop with its layers and masks concepts that for the first time allowed non-destructive editing Apple with their trash concept that allowed tolerance for errors, and thus allowed undeleting The spreadsheet and its formula concept, that allowed flexibility in any kind of calculation Calendly and its concept of event type that allowed the do-it-yourself of appointment taking The World Wide Web and its URL concept, that allowed easier accessibility of resources The difference between apps that offer similar functionalities can be highlighted by the concept they are built upon.\nText messages and email clients are both used as a mean of communication, but the former is built around the concept of conversation, and the latter on concepts like mailbox and folders that elicit different types of communication styles.\nConcepts also describes whole families of apps: text editors like emacs, vi, and so on converge around the line and the character, while word processors use the paragraph as their main concept.\nConcepts define business By defining the concepts and the way they could interact, we could envision a roadmap of product releases, from an MVP to a full-fledged product, considering the cost-benefit ratio.\nEach concept outlines a different purpose, and can be researched and developed by different teams that can work in parallel. Only when it‚Äôs time for composition, the incompatibilities can be reconciled.\nBy shaping concepts in a certain way, companies can expose (or hide) their business model to users (think about the pain of booking a flight!: legroom, seat types, and so on: the way those details are hidden from the users is a precise business decision to make the market trickier).\nConcepts expose complexity Think about the browser and the concept of certificates. How many normal users are aware of their importance? Could a better concept at the base of the authentication have led to a safer internet environment?\nIf we focus on those troubling parts we might expose the ‚Äúreal meat‚Äù, forgetting about the details, and this is important both for teaching/documenting and for focusing on refactoring to build better apps/software.\nConcepts ensure safety and security We knew of fatal accidents due to a misinterpretation of unit of measure in medical treatment. There was a misconception of the fact that unit of measure are depends on culture and context, for a starter.\nSimilarly, ‚Äúreal‚Äù security cannot be enforced as a patch to an already established workflow.\nSecurity concepts and their vulnerabilities should be first citizens at conceptual level.\nExpressing concepts What is the appropriate language to talk about concepts, and how can we express them?\nLet‚Äôs examine the Mac trash concept.\n1**concept** trash [item] 2**purpose** 3\tto allow undoing of deletions 4**state** 5\taccessible: set item 6\ttrashed: set item 7**actions** 8\tdelete (x: item) 9\twhen x in accessible but not trashed 10\tmove x from accessible to trashed 11\trestore (x: item) 12\twhen x in trashed 13\tmove x from trashed to accessible 14\tempty() 15\twhen some item in trashed 16\tremove every item from trashed 17**operational principle** 18\tafter delete(x), can restore(x) and the x is accessible 19\tafter delete(x), can empty() and then x not in accessible or trashed The name of the concept is followed by a list of types that can be ‚Äúspecialized‚Äù when the concept is instantiated (an item can be trashed).\nThe purpose outlines the main goal of the concept and sometimes can be subtle. For instance, the Mac trash icon was the first to allow the undoing of deleting (there were also plenty of ways to delete files). So that was the real killer feature and the one outlined here in the example above.\nThe state answers the question: what changes will be applied to what types? In other words: what should be remembered by the concept during execution?\nIt‚Äôs possible to design a concept where we reach a state in which no action is allowed. This is called deadlock, which is not desirable because it‚Äôs a state we cannot escape.\nActions show the behaviour of the concepts, and are instantaneous ‚Äîthey take no time. They can have preconditions and set states. They are deterministic. It‚Äôs the state of the preconditions that determines what actions are available at any given time. The actions can be seen as state machines. ‚ÄúMathematically, the meaning of an action is a relation‚Äù.\nThe operational principle show how the purpose is fulfilled by the actions, by outlining some archetypal scenarios. It doesn‚Äôt add anything that can be inferred from other sections, but it helps understand the how and the why. It can be thought of a theorem about the behaviour of the concept.\nKey questions to ask Here‚Äôs a list of questions to ask ourselves while devising concepts, in no particular order:\nWhat are the key concepts? What the most valuable? The most troubled? Are there shared concepts? Have concepts changed with time? What are their purposes? Are we missing some? What about the competitors‚Äô concepts? How are composed? How‚Äôs the synchronisation? And concept redundancy and /or overload? Are concepts effectively mapped to the user interface? Is the documentation organised around concepts? What set of concepts form an MVP? What problems this concept solve? Can we avoid reinventing the wheel by using already developed libraries, or just adapting one of those? Can a concept map to a module so that they can be loosely coupled? Concept purposes ‚ÄúIt‚Äôs not enough to know why you‚Äôre designing a product. You need to have a why for each element of your design, a purpose for every concept‚Äù.\nPurposes and goals are different. A goal is a thing a user wants to obtain with the help of a concept; a purpose is the motivation because the designer chose to include that particular concept in the software.\nPurposes should be:\ncogent must express a need for the user. ‚ÄúSave a favourite page‚Äù (no!) ‚ÄúMake easy to retrieve pages for later use‚Äù (yes!). specific evaluable It should be clear whose purpose we are describing. If a concept lacks purpose, it‚Äôs because there wasn‚Äôt a real user need behind: it was built in that way only because it was easier.\n‚ÄúConcepts, unlike internal mechanism, are always user-facing, and must have purposes that make sense not only to the programmer but also to the user‚Äù.\nIf a concept‚Äôs purpose is misunderstood, it will likely be misused.\nGiven that concepts are not formalisable, testing is essential. Moreover, while a full list of requirements is untenable, a list of requirements to avoid‚Äîmisfits ‚Äî is feasible.\nConcepts without purposes are rare, but can arise from exposing to the user mechanisms that should have been hidden.\nPurposes without concepts may indicate constraints that originate outside the designer‚Äôs domain, or sometimes just egregious omissions.\nConcept overloading A concept with more than a purpose is overloaded. It can be caused by:\nFalse convergence: a concept designed to cover two functions wrongly thought to be aspects of the same concept. This can be avoided by better analysis / design. Denied purposes, despite users‚Äô requests. The remediation here is simply to listen to the users! Emergent purposes: those that are born from old concepts, often by the users themselves: the hardest to avoid, the best approach is to always stay alert and acknowledge when they occur, to address them fast and hopefully well. Piggybacking:happens when a concept is used to accommodate a new purpose (by design, not by the users). This is avoided by recognising that concepts reuse typically leads to later costs that cannot be forecast. For instance, the Facebook like is overloaded: can signal reaction, curate your feed, target your advs.\nConcept composition Concepts are independent, but we can take some and compose into an application, by synchronising their actions.\nThis will generate some dependencies, that however do not belong to the concepts per se, but are a feature of the composition process, the context.\nComposing concepts do not change the behaviour or the concepts they comprise (concept integrity).\nWe distinguish three types of composition:\nThe free composition is when concepts are merged in a single product, but operate independently and the only constraints are those impose by the concepts themselves (you can‚Äôt eat a meal that hasn‚Äôt been prepared, for instance).\nThe collaborative composition links concepts to achieve functionality not provided by any of the composing parts. For instance, a composition of contact and phone call, that allow the composition to be treated as a sequence of stages.\nThe synergistic compositions occur when the overall value of the composed parts bring more value than the sum of the singles parts. For instance, the trash is a mix of the folder concept and list of items concept. Most synergies, however, will have some costs: the trash folder is a ‚Äústrange‚Äù folder because it has the empty button‚Ä¶\nWhen we tightly synchronise concepts, we trade flexibility for automation:\nSynchronising too much takes control away from the user, preventing some scenarios that would have been allowed in a free composition. Synchronising too little, conversely, burdens the user with work that might have been automated.\nConcept dependence Composition can introduce an asymmetry in the way a concept augment the functionality of another.\nIn a to-do app, we can compose the task concept with the label concept. But nobody would start thinking about a to-do app with a label concept to which attach a task.\nConcepts are always independent, but considering the product as a whole they can assume a configuration in which some depends on others.\nSometimes, a concept can be justified by several other concepts. It‚Äôs important to separate the primary dependency (e.g. a user concept and Q\u0026amp;A) from the secondary dependencies (e.g. the upvote because we could also use the authentication to prevent double voting). Secondary dependencies could be omitted or developed in phases.\nDependencies tells us how to avoid introducing a concept before it can be motivated.\nTo explore ways in which an app might be simplified, evaluate the consistent subsets and estimate how much value each one brings. Perhaps there is a subset that brings most of the value for only a small part of the cost.\nConcept mapping (from concepts to UX) Interface design has for long being the realm of HCI experts, bridging the physical and linguistic levels with that of widgets. Bringing concepts to the table, we can consider buttons as a way to activate the actions subsumed by concepts and the visualisations a way to communicate the current state of the concepts.\nSo a concept mapping is a translation table from a concept to its material form.\nIt is at this level that naughty things can happen: by hiding or making very hard-to-access information, you can nudge your audience.\nObjects (from concepts to OOP) üí° Objects (or items) receives the actions and the change of states.\nObjects roles Classifying objects by roles can be useful because rise questions about its property, uniqueness, management‚Ä¶ it raises awareness. Objects can then be viewed:\nas an asset: it thus has inherent value, that might be different between users and also for the same user at different times and for different purposes as a name, to locate or identify other objects. I.e. a serial number names a camera, and email address names an email account. It points to only a resource, but its interpretation of what resource depends on the context. as a value. They assume meaning only in relationship with others objects. I.e. the number 80. Objects and mutability ‚ÄúSince concepts communicate only by synchronisation of actions, objects that are passed as arguments of actions are required to be immutable. If they were not, an action in one concept might mutate an object shared by another, leading to a hidden communication.‚Äù\nFor the reservation concept, for instance, the user might think we have a mutable reservation object.\nInstead, we have a reservation relation, which is a tuple of states in the reservation concept. The change can then be seen as a transition from an immutable relation X to an immutable relation X‚Äô by exercising an action.\nInterpreted/uninterpreted objects If we need to pass a date, we should agree on the format, and this agreement is an interpretation.\nIf we need to cancel John‚Äôs booking, we only have to pass John‚Äôs name, without further knowledge (uninterpreted).\nIn his 5-pages-long note 44, Jackson gives us a formal explanation of this concept in terms of permutation invariance:\n‚ÄúTo check whether a type is interpreted, one need only look at the operations performed on its objects in the state updates of the actions: if they include only the ‚Äúlogical‚Äù operations of sets and relations and equality comparisons, the type is uninterpreted‚Äù.\nConclusions In conclusion, Daniel Jackson‚Äôs The Essence of Software constitutes the missing link between the qualitative study of human behaviour and the rigorous demands of software engineering.\nBy providing a formal language that pushes us to be more precise, we can concentrate on the conceptual level, thereby eradicating design flaws at the source rather than merely mitigating them in the implementation.\nI would recommend this book to anyone curious enough of software-related topics because the methodology exposed in the book*, which benefits from every kind of expertise, establishes concept thinking as a fundamental, cross-disciplinary craft.*\n"
},{
    "uri": "http://localhost:1313/books/radically-candid/",
    "title": "Radical Candor: Be a Kick-Ass Boss Without Losing Your Humanity",
  "tags": ["feedback","communication","kim-scott","business"],
  "body": "In this article I want to talk about ‚ÄúRadical Candor‚Äù, whose subtitle is ‚ÄúBe a kick-ass boss without losing your humanity‚Äù, by Kim Scott, a book originally published in 2019.\nBut wait, I am not a boss, so why should I care? Because this approach is relevant even if you, like me, are part of a team.\nThere are two main pillars in radical candor:\ncare personally: You don‚Äôt have to be just professional. That‚Äôs the minimum. You should show that you care about your interlocutors as human beings.\nchallenge directly: you should give feedback to people, even when it‚Äôs negative feedback. And precisely because you care about them as human beings you need to give that feedback, because otherwise how can they move forward? Feedback coming from a person you trust is received with much more credit and less bias and this contributes to nurturing the caring environment we are envisioning. Besides that, the person receiving the feedback will try to adapt to the requested changes (if any) and will be encouraged to give feedback to others, too.\nSee? The only prerequisite here is being a human being, not being a boss.\nTogether, care personally and challenge directly are a way to offer guidance.\nWhy did the author call this approach Radical Candor?\nRadical because there are many people used to not saying what they really think.\nCandor because the key is getting people used to direct communication so that there cannot be misunderstandings.\nIt‚Äôs an approach that is very context-dependent: It‚Äôs ‚Äúrelationally and culturally aware‚Äù (p. 308). What worked in a company, in a country, won‚Äôt necessarily work in another.\nAt the heart of this approach, is, of course, the respect of the other. You don‚Äôt hold the good points and the others the bads. You should have an open-minded attitude because you will encounter people with different worldviews and values, and you‚Äôll have to care personally for those people too.\nWhen you are lacking the care when giving feedback, you‚Äôre in the Obnoxious Aggression quadrant. When this is a reflection of ‚Äúbosses should act like tyrants and employees like slaves‚Äù of course criticism is not a tool for improvement and can lead to toxic environments.\nSometimes, however, it‚Äôs better to offer rude feedback than to fall in the other two quadrants (be insincere or too emphatic). Remember, however: don‚Äôt blame the people (their internal essence), blame their behavior!\nManipulative Insincerity happens when you give feedback (or not) with your own purposes in mind, to be liked or to spare you some emotional job because you are too tired, maybe.\nThe focus switches from the other person to your own personal gain.\nThe other quadrant is Ruinous Empathy: when you prioritize ‚Äúbeing nice‚Äù instead of critique, you fail to give a chance to improvement.\nKeep in mind that the candor the author refers to concerns the domain of business; ‚Äúrelationships require some privacy, so while I am all for transparency when it comes to business results, I don‚Äôt believe that Radical Transparency fosters good working relationships, contributes to psychological safety, or results in a productive, happy culture‚Äù (preface, page XII).\nHow to accept criticism When accepting criticism, remember: you are there to listen and understand: repeat what they are telling you to clarify what they meant, ask questions, but do not defend yourself (it‚Äôs not the correct setting!).\nIf you agree with the feedback, act accordingly, making a change as soon as possible or at least, if things need more time to be accomplished, make something visible to let your criticizers know you rewarded what they told you.\nIf you don‚Äôt agree with what you heard, commit to another meeting, where you and the others will talk in-depth about your motivations and you‚Äôll give space to others to talk back.\nIgnoring feedback is not respectful.\nHow to offer feedback You should describe:\nthe situation: try to be precise and concrete the behaviour the impact you observed That‚Äôs all. Don‚Äôt be personal or subjective. This applies to both critiques and praises.\nGive feedback as soon as needed, when memories are fresh enough. Remember that, even if you are not personal (i.e. you‚Äôre saying that‚Äôs wrong and not you‚Äôre wrong), the receiver of the critique will feel it as personal, because we spend a lot of time working together. So, remember to care personally and be compassionate!\n"
},{
    "uri": "http://localhost:1313/books/programmers-brain/",
    "title": "The Programmer's Brain: What every programmer needs to know about cognition",
  "tags": ["cognition","learning","code-reading","felienne-hermans"],
  "body": "I found The programmer‚Äôs brain, by Felienne Hermans very interesting. The only drawback, in my opinion, was the organization of contents: I would have rearranged in a different manner because in the way I read them I felt some low cohesion. Below I won‚Äôt cite too many researchers and professors, but keep in mind that the book has all the names and the bibliographic information needed!\nActivities in programming According to the Cognitive dimension of notation framework, the act of programming comprises five activities:\nsearching for a specific information comprehension transcription - just coding (you are transcribing your understanding, that is ‚Äúyour mind‚Äù, into the code) incrementation: a mix of searching, comprehension and transcription exploration: when you have a vague idea of where you want to go, so you need to ‚Äúplay with code‚Äù in order to get insight and gain clarity about further directions Debugging is not listed because it is seen as a mix of all five activities.\nEnhancing our comprehension with easy-to-read code Research shows that 60% of a programmer\u0026rsquo;s time is spent understanding rather than writing code. But we are not so good at reading code and this sometimes leads us to reinvent the wheel because it‚Äôs simpler than to invest time learning how to be efficient on reading code.\nWhen we read, we leverage our short term memory whose size is limited. Several psychological experiments have found its size to be 5/7 items or even smaller. What is precious here is the concept of ‚Äúitem‚Äù. That‚Äôs because its size can be changed from smaller to bigger with training and the powerful technique of chunking. That‚Äôs why expert developers are better at reading code.\nBut there‚Äôs also the iconic memory: the sight‚Äôs sensory memory, just before it becomes conscious information: of course not all you see can enter your short term memory. But you can also leverage the things that do not emerge from this (unconscious) level.\nSo, how to write chunkable code that can leverage iconic memory? The author reminds us the importance of using design patterns, writing high-level comments and leaving beacons in the code. Beacons?\nBeacons provide triggers to confirm or refuse hypotheses about the source.\nSimple beacons are syntactic code elements, such meaningful variable names.They are simple enough to unlock the functionality of the code by themselves. Compound beacons are larger structures comprised of simple beacons providing semantic meaning for functions that simple beacons execute together. Forgetting Curve It takes 15 minutes to get back to work after an interruption. If you think about that, that‚Äôs a good reason not to rely on the internet (and instead invest on studying syntax) when dealing with new programming languages. The author describes a method involving flash cards and then illustrates Herman Ebbinghaus‚Äôs Forgetting Curve. One trick to try to overcome this forgetfulness is repetition, but not at regular intervals: those intervals should last longer, because as the memory settles in our long term memory the kind of training it needs will be different: from continuous and numerous repetitions needed to establish the memories to repetitions needed to avoid the links to memory networks to disappear: this kind of training will improve the storage and retrieval strength.\nOne key to counter the forgetting curve is the elaboration.\nThis article you are reading is an elaboration, which means, by the words of the author,\nthinking what you want to remember, relating it to existing memories, and making the memories fit into schemata already stored into your LTM (p. 44). An elaboration is a way to restructure your understanding and to sculpt it into a long term network of memories.\nCognitive load Joan Sweller identified three types of cognitive load:\nintrinsic cognitive load: how complex is it the problem? extraneous cognitive load: what outside distractions add to the problem germane cognitive load: cognitive load due to the handling of finite cognitive resources (= brain). For instance, when we are so deep in the calculation that, when finished, we forgot the problem at hand. We should train ourselves to distinguish these three types of cognitive loads because to address them we need different strategies: for instance, we may have and an extremely convoluted code (suffering from extraneous cognitive load) that would benefit from a major refactoring: in that way the intrinsic complexity of the problem would surface while all the other ancillary problems would be encapsulated in other libraries/classes and thus out of sight.\nMental models in a developer‚Äôs world Mental models can help you solve problems and help you ease your cognitive load. They are like\nan abstraction [of the world] that help you reason about the problem at hand (p. 95)\nMental models are by nature incomplete and can change over the course of time. Besides, several mental models (regarding related issues) can coexist, each with its local coherence, even if - taken as a whole, it would appear as totally incoherent. Finally, we should remember that sometimes we could fall back to old, falsy mental models, because once new ones are established the older are not simply wiped out, but kept in our mental storage (long term storage) until oblivion or recall.\nAll that said it becomes evident that data structures, design patterns, architectural patterns, diagrams, modeling tools‚Ä¶ all can serve as mental models. Double check the metaphors you will be using, though. They might create confusion.\nHow to onboard new developers One of the reasons more-senior people often struggle with effectively teaching and explaining is the \u0026ldquo;curse of expertise‚Äù. Once you have mastered a certain skill sufficiently, you will inevitably forget how hard it was to learn that skill or knowledge. You will, therefore, overestimate how many new things a newcomer can process at the same time. (p. 206).\nExperts are not the same as junior developers, only faster. Instead, their gained experience made their long term memory different from junior developers and so their approach to problem solving is a lot different from them.\nThere are three kind of confusions at play when dealing with all the cognitive activities related to software development:\nlack of knowledge, related to long term memory (LTM): for instance, when you gain experience of a programming language structures and its idioms, long term memory plays a role because it cements the knowledge (you don‚Äôt have to think about i.e. its loop structures). It‚Äôs kind of hard drive. lack of information, concerning short term memory (STM): the short term memory is activated for instance when keeping note of variable names. It‚Äôs called RAM. lack of processing power, regarding the working memory. The working memory is triggered for instance when you are mentally tracing the working of a code. If you feel the urgence to take notes it may be a sign that your working memory is overloaded. That‚Äôs CPU. This is particularly true for new hires, and that‚Äôs why it would be a very good outcome if we could introduce this psychological outcome in our lingo: saying things like ‚Äúmy working memory is overloaded‚Äù would communicate much more information then that ‚ÄúI‚Äôm confused‚Äù!\nBesides that, what‚Äôs most certainly true is that new hires are asked to do too much at the same time. The simple tasks they are assigned will probably deal with all but exploration phases all the same time learning other important stuff like the company organization, the team way of working, and so on.\nIn order to ease the life of the new hires (and, If I may add, to better evaluate their specific skills) the author suggests preparing specific smaller tasks - each one that can be built on top of the previous - that focus only on one aspect of the programming, as defined in the cognitive dimension of notation framework.\nAnd remember:\nunderstanding is a better welcome task than building. (p. 216)\nAutomatization In our long term memory resides two kind of memories:\nthe procedural (implicit) memory which is the memory that describes how to do the things: for example how to tie your shoelaces, how to ride a bike. Remember how you learned? At first by paying attention to every single aspect of the task, until, repetition after repetition, it became second nature and the thinking became ‚Ä¶ implicit. the declarative (explicit) memory concerns episodic memories, things that happened in the past. Researchers demonstrated that experts rely heavily on episodic memory when trying to solve problems: in a sense, they recreate familiar settings and apply the same reasoning that worked that time. But what about a solution in which we depended a great part on the procedural memory instead? One of the great aspects of the book is that is it full of exercises to help improve our cognitive fitness. In this case the author call it automatization:\nthis automatic performance of the task is quick and effortless because retrieval from memory i s faster than actively thinking of the task at hand can be done with little or no conscious attention. (p. 170).\n"
},{
    "uri": "http://localhost:1313/books/philosophy-of-software/",
    "title": "A Philosophy of Software Design",
  "tags": ["design-principles","complexity","software-architecture","john-ousterhout"],
  "body": "Please note: I won\u0026rsquo;t publish a direct link to the page details here since I cound\u0026rsquo;nt find the publisher\u0026rsquo;s page. Just google the title to find out where to buy the book (highly advised :)\nJohn Ousterhout, as his Wikipedia page states is a professor of Computer Science at Stanford University. He also invented TCL in 1988.\nJohn decided to write this book because he found that the most demanding process in computer science was problem decomposition (that is how can we split a complex problem into smaller, manageable and developable parts?). Yet nobody teaches this skill: everybody builds on that, taking for granted that anybody can do that, as an innate feature.\nThe author thinks that designing skills are what separates a great programmer from average ones.\nHis mantra, emerging throughout his work, is: reduce complexity.\nOusterhout is also humble; he knows very well that his book derives from his own opinions and that we might disagree with his view. He invites us then to take his suggestions with a grain of salt. I found this very refreshing: there are a lot of books out there that provide us the real-truth, in every human field!\nOn complexity, dependencies, and obscurity Complexity is everything that makes it hard to understand and modify the software.\nSigns of complexity include:\nChange amplification: a supposed simple change requires a lot of updates in many different parts Cognitive loads: how much a developer needs to know to complete a task? Unknown unknowns: it is not clear what should be done in order to achieve our goal The causes of complexity are either dependencies or obscurity.\nWe cannot really create a software system without adding some dependencies, but our job is to make it explicit and reduce their number to a minimum.\nObscurity, defined as anything that is not obvious or not consistent, is clearly subjective. So we cannot evaluate it by ourselves, we need code reviews.\nOn programming approaches We should understand that complexity is an emergent quality. It is something that accumulates by every little thing we skip to the next time. Every little thing that we leave unresolved will settle creating a layer of dust into our system. As the dust grows, our system becomes dirt. But when exactly did it became dirt? You cannot tell the real moment: cleanness, as complexity, is a gradient. And it has no end!\nThat‚Äôs why there‚Äôs a chapter called ‚Äúworking code isn‚Äôt enough‚Äù. Tactical programming (that is ‚Äòget those features as quickly as possible‚Äô) won‚Äôt work in the long run. This is how systems become complicated.\nAnd this is somehow different from well-established principles as YAGNI or KISS had always told us\u0026hellip;\nIn order to avoid complexity, we always need to keep in mind the long-term structure of the system, an approach the author defines as strategic programming. It‚Äôs rare, nowadays, that we create the software from scratch: we almost always update existing code. So we should program in order to ease future code extensions: this will facilitate our future developments. Ousterhout suggests investing about 10%-20% of the time planned on a task to produce better software design. This will make completing tasks 10%-20% slower, but only in the first runs: since once the complexity is reduced, we can take advantage of that.\nAnd this resembles the boy scout rule\u0026hellip;\nOne advice the author gives us is to try to design every system twice, approaching the problem from different standpoints. No one gets it right at the first try!\nOn deep modules The book then goes on by defining deep modules. A module is a unit of code with an interface and an implementation.\nDeep modules are those that provide powerful functionalities with simple interfaces.\nModule‚Äôs interface represents the complexity that the module imposes to the others. Simpler is (of course) better!\nWith this perspective, shallow modules are the ones that expose quite long interfaces (in comparison with their implementation), such as small modules.\nSide note: This is somehow original because we are always taught that small modules are good and one of the refactoring goals is to split long class into several smaller ones. This approach, however, may lead to cognitive load: since every module does not contribute so much in itself we will need a lot of them, each with its own interface to digest.\nThe author is, instead, not against long methods. They are fine if they have a simple signature and are easy to read.\nMethods, in other words, should do one thing and do it completely.\nThe decision to split or join modules should be based on complexity. Pick the structure that results in the best information hiding, the fewest dependencies and the deepest interfaces.\nOn information hiding and leaking How can we get rid of complexity? One technique is information hiding. It will simplify the interface and make it easier to make amendments. The opposite of information hiding is information leakage and could occur in several ways: via explicitly declaring a dependency in an interface or, worst, if a set of modules depends on assumptions about the outside world (say, for instance, we have two modules that both rely on a file format). Another kind of information leakage is the temporal decomposition: in this case, we have functionalities that should happen in a given order, split into several modules or classes. Of course, we should hide only that information that is needed only inside the module. Our job as developers is to minimize the information that needs to be let outside the module.\nOn interfaces The book goes on suggesting several techniques to reduce complexity. One interesting advice is to create a specific module for the given task but with a generic interface. As the author says,\nthe interface should be general enough to support multiple uses: the interface should be easy to use for today‚Äôs needs without being tied specifically to them.\nOn software layers A software system is composed of different abstraction layers. Ideally, methods directly implementing the interface should be at the highest level of abstraction and make use of lower ones in order to achieve their goals. If we found ourselves up and down this lattice we might have a problem with class decomposition. We might have a problem, for instance, when facing pass-through methods (from one class to another) if they contribute no new functionality. A dispatcher, for instance, is a pass-through method, but It has a reason to exist. Decorators (as per the author‚Äôs point of view) seldom add new functionalities and usually, we can avoid them with other methodologies. And how can we evict pass-through variables? Ousterhout suggests using a context object, an object which will contain all the needed pass-through information without recurring to global state (which is, indeed, bad). The author suggests passing that context object only to the constructor and then storing a reference in the class in order to reduce the pass-through of that single object.\nPersonal note: I would, however, stick to this (that is, I wouldn‚Äôt save the context object in the class state) since this will re-instate the problem of object state inside the class and making the class methods less testable. I think that reducing the clutter of pass-through variables from a bunch to one is a good result!\nOn Exceptions Exceptions add complexity because we should decide whether to handle them and where (at the low level or at the outer most?) because writing the handling methods could lead to other exceptions and also because it is difficult to test if such code really works. One approach is to define errors out of existence. For example, instead of throwing an error in case we are trying to unset an unknown variable, we can simply do nothing, in that case. In the same way, we can make special cases out of existence. The author gives this example: think about a text editor and the text selection mechanism. Instead of cluttering all the code of if statements testing if a selection is available we can make the selection always present by having it at length 0 when the user did not select anything. In this way, we can let exceptions handle only exceptional things!\nOn comments Without comments, you can‚Äôt hide complexity. ‚Ä¶ Comments, if done correctly, will actually improve a system‚Äôs design. If a user must read the code of a method in order to use it, then there is no abstraction.\nIdeally, developers should read only the interfaces. To help them, comments are needed to describe what‚Äôs not obvious. Typically, comments will add information at a lower level of the code (adding precision) or at a higher level (adding intuition). Comments at the same level of the code are likely to repeat the code itself and thus be unuseful. Try to make it easy to find documentation (put comments near the code they describe) but do not repeat comments; instead reference it, if needed.\nOn consistency Consistency means that similar things are done in a similar way. This ease the cognitive burden, because once you have learned how to achieve a goal, you‚Äôll have established a habit to re-use on other occasions. If a system is not consistent, two situations may seem the same when actually they aren‚Äôt, leading to avoidable mistakes.\nOn obviousness A code is obvious only if the current reader is able to read it and comprehend it. You should provide all the needed information to correctly digest your code. You can do this in three ways:\nreducing the information needed, eliminating special cases and leveraging abstraction taking advantage of consistency, you can meet your readers‚Äô expectations presenting important information right in the code with techniques such as giving good names to methods and variables and in-line comments. On Agile Agile development focuses on features, and once is done, moves to the next. Agile development, in fact, dictates that development should be done incrementally. Ousterhout suggests that\nthe increments of development should be abstraction, not features.\nOn test-driven development TDD focus attention on getting specific features working, rather than finding the best design: it‚Äôs tactical programming pure and simple. ‚Ä¶ At any given point, it‚Äôs tempting to just hack in the next feature to make the next test pass.\n"
},{
    "uri": "http://localhost:1313/books/living-documentation/",
    "title": "Living Documentation: Continuous Knowledge Sharing by Design",
  "tags": ["documentation","ddd","knowledge-sharing","cyrille-martraire"],
  "body": "A little confession and the need for something better I confess: I like to write documentation. I admit: I am (sometimes) dedicated to the antipattern: human dedication (see page 32). Sometimes I need to sketch up things while following the execution of the code, and it‚Äôs a pity to throw away those findings. Other times I need to clear up my mind over some convoluted code. In any case, I resort to the tool that the company I work for offers: Confluence pages. Once I‚Äôve finished writing the article, that article begins to age, eventually reaching, as the author says, the information graveyard. If I‚Äôm lucky enough, I can intercept changes to the code and amend the docs accordingly. But not everyone is so fond of writing documentation. And that‚Äôs where this book that you can buy here comes in handy: ideas on automating the generation of documentation.\nWhat this book is about and what it is not This book is not about the end-user documentation, nor does it aim to bookish-style, well-refined output. This book is about sharing of knowledge and making it available in the right place, right on time. By doing this, it hopes to shed a light on the overall design of the application being documented, eventually triggering an opportunity for refactoring. Overall, it wishes the code to be so clear and self-documented that the additional documentation becames useless and redundant. And that‚Äôs why this is not a book about end-user documentation: not every audience is able/willing to read the code, nor that every audience should do it.\nThe need for documentation Documentation supplements the knowledge we might not have. Lack of knowledge manifests in\nwasted time (finding the missing points or guessing them) biased decisions due to this lack Hint: when you don‚Äôt know something, you usually don‚Äôt know of not knowing ;) So the time spent on harvesting knowledge should be considered helping build the stakeholders‚Äô application mental model. And this is important because it is that model that developers will use to augment the code, product owners will use to describe the stories to implements, and business owners will use to describe their objectives and key results‚Ä¶. Let‚Äôs hope that every stakeholder has a shareable and overlapping mental model!\nWritten documentation is not always the correct answer The book follows the track of the agile / TDD / BDD development and builds on their pillars.\nWith TDD, the tests are first considered as specifications. With DDD, we identify the code and the model of the business domain [‚Ä¶]. ‚Ä¶ we expect the code to tell the whole story about the domain. (p. 12).\nSince the purpose of the documentation is a transfer of knowledge, in many cases the preferable and more convenient way to achieve this is by having conversations, which is another way of saying The most efficient and effective method of conveying information to and within a development team is face-to-face conversation(Source: https://agilemanifesto.org/iso/en/principles.html)\nFact is that even when we favour ‚Äúconversation over documentation‚Äù, sometimes we need to summarize that we have discovered. That summary will be less wordy and up-to-the-point since all the assumptions and agreements have already been discussed. (For instance, we might have decided to adopt the hexagonal architecture. Via some conversations, we might have discussed it amongst several alternatives. In the documentation, we can explain why we chose that architecture and the rationale for having discarded the other choices, without discussing what hexagonal architecture is. This is in fact a well-established knowledge that is not worth documenting again: to establish a common ground for those readers who haven‚Äôt got the chance to participate in the conversation we can effortlessly link to an external resource). Hint: as the author says (p. 13): we have conversations even with machines, and we call that experiments.\nWhat to document (in a written form) When in doubt about its usefulness, don‚Äôt document. If some information will be relevant for a very long period of time, document it. If some information should be spread to a large audience, document it. If some information is considered valuable or critical, document it. Anything that makes an application hard or expensive to change should be documented Avoid speculating on what should be documented. Instead, pay attention to all questions asked or questions that were not asked but that should have been asked, as signals that some knowledge needs to be documented. (p. 282)\nKnowledge is around us (and in us) One aspect the book states clearly and several times is that often the documentation is already there, you need only to recognize it as a source to draw information from. Consider for instance the followings:\nTests written for behavior-driven-development Configuration files Some README.md that you might have written times ago Annotations you already use for documenting routes, or for your ORM (like Doctrine) The point is that every kind of information that ‚Äúdecorates‚Äù the code, gives it some context, and functions as a way to transfer knowledge (even from yourself of today to yourself of the future) is documentation. Moreover, since most of this stuff always needs to be up-to-date because it is needed by the application to run (or to deploy), you can be assured that it will be always accurate and can become the single source of truth. If anything, the problem of this kind of knowledge is its fragmentation and / or obfuscation. Not to mention the worst case scenario, that that knowledge is only in the developer‚Äôs mind and in the code we can read only its consequences.\nDocumentation accelerates delivery because it shortens the time to rebuild your mental model of the system to work on. (p. 341).\nInternal vs external documentation The author distinguishes between internal documentation (placed in the code, for example via annotations or codes or near it, i.e. with README.md files) and external documentation. This kind of documentation can be leveraged even with the most sophisticated IDEs. External documentation is something built from internal documentation but:\nembellished (maybe following the company‚Äôs guidelines, with the company logo, and so on) filtered: not every documentation should pass the code level because sometimes information is relevant only at a micro-level but becomes useless or distracting at upper levels. Moreover, what will be considered important for a particular audience would be noise for another. sometimes transformed to speak the language of the targeted audience (thas made more readable) published somewhere (thus made more searchable) So, what‚Äôs living documentation? Living documentation is documentation that it is not a burden to maintain and that is always kept in sync with its sources of truth, eventually failing the CI/CD if they fail to synchronize. Once the documentation is extracted, it can also be published in a format of your choice (say via Confluence API). Of course, no one will ever automate the first step: writing and/or assembling the automatable documentation.\nDevelopers don‚Äôt own the documentation, they just own the technical responsibility to deal with it. (p. 22) Living documentation is all about making each decision explicit, with not only the consequences in code but also the rationale, the context, and the associated business stakes expressed (or perhaps modeled) using all the expressiveness of the code as a documentation medium. (p. 50) Anything that can answer a question can be considered documentation. If you can answer questions by using an application, then the application is part of the documentation. (p. 199)\nMoreover, sometimes the application is the only source of truth: think of legacy systems where no one have prior knowledge of them. But\ndon‚Äôt fall for the fallacy that the legacy system is in itself a sufficient description of the new system to be rebuilt. Take the opportunity of the rewriting to challenge every aspect from the legacy system. (p. 403)\nNot everything should be made alive If the burden of implementing the automation is not worth the time it takes, because the knowledge won‚Äôt change much, maybe the traditional way is preferable. In other words: keep it simple, stupid. The author defines such kind of documentation ‚Äúevergreen documentation‚Äù (see p. 246).\nSome guidelines and suggestions Only one single source of truth is allowed: if you need to make it accessible somewhere else, use references (i.e. links or annotations that, in the publication phase, will copy from the source on the destination). Of course, with every publication, we provide an up-to-date snapshot of what was current at the time of the information harvesting. That‚Äôs why we should clearly version the documentation every time we publish it. Every published document should be considered read-only: if you need to amend it, publish another version. Use annotations to augment your code: the code tells the how, the annotations could tell the why, aka the rationale (see p. 122) Annotations can be used to mark security issues (@securityIssue(name=‚ÄôblaBla‚Äô, jira=‚Äô‚Äô), or to declare that a class should have not setters Stick to one of these styles: declare @ Immutable, @ NotNullablel vs @ Mutable and @ Nullable. Don‚Äôt mix styles to avoid inconsistencies. [I put a space between @ and the annotation not to tag anyone ] Annotate elements with only intrinsic information (to explain this, the author uses the car example: its color and engine are intrinsic properties, while its location and owner are extrinsic, see p. 118). When used this way, if an element disappears from the code, all the relevant documentation goes with it, without side effects in other parts of the documentation. If you link something directly in your documentation, you should prove their existence in life every time you publish your documentation. Another approach is to link to a google search that likely will show your desired first choice at the top of the list Consider a link registry You can use annotation also to highlight some best practices you find on the code You could create a glossary out of your entities or entity properties, maybe (but don‚Äôt create a Glossary annotation, use instead CoreConcept, see p. 164) You could create guided tours of particular sections of your code You could laso integrate some diagrams (see Graphviz and ‚ÄúLiving diagrams‚Äù and following, page 170). Don‚Äôt use primitive types, create your own: types don‚Äôt lie, and if you don‚Äôt use them properly, the pipeline will be red! Use tools to enforce the decisions made into guidelines (p. 301): they provide knowledge just in time where is needed (when someone violets them). Conclusion That‚Äôs (a lot) more than living documentation inside this book. It goes on talking about code quality and code design techniques: for example, even when naming a method we could introduce a burden on the shoulders of the next developer or alleviate her voyage. It touches code organization (aka, in which folder should I put this kind of files?), refactoring, architecture, agile, test-driven development‚Ä¶ I think it touches a lot of the aspects that make software development a craftsmanship. And I think several kinds of audiences would benefit from reading this book. Besides developers, I think product owners would benefit because, apart from answering the questions ‚Äúwhy we should document?‚Äô and ‚Äúwhat?‚Äù, it gives hints on how to write stories on Jira (which are indeed, some kind of documentation too). I think some aspects touched in the book (decision logs, for instance) could be applied to any level of a company, of course with different kinds of granularity. In summary, I really loved this book and would recommend it to anyone even slightly interested in documentation. If you‚Äôre like me, you‚Äôll end up full of ideas‚Ä¶. and the book gives also advice on how to sell the living documentation approach to your colleagues and boss :)\n"
}]